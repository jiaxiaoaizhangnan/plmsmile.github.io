<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>语言模型和平滑方法 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="自然语言处理,各种熵,语言模型,数据平滑">
    <meta name="description" content="语言模型二元语法$ $ 对于一个句子$s=w_1 \cdots w_n$，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 $$\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|w_{i-1})">
<meta name="keywords" content="自然语言处理,各种熵,语言模型,数据平滑">
<meta property="og:type" content="article">
<meta property="og:title" content="语言模型和平滑方法">
<meta property="og:url" content="http://plmsmile.github.io/2017/07/31/nlp-notes/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="语言模型二元语法$ $ 对于一个句子$s=w_1 \cdots w_n$，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 $$\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|w_{i-1})">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-11-25T08:30:10.541Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="语言模型和平滑方法">
<meta name="twitter:description" content="语言模型二元语法$ $ 对于一个句子$s=w_1 \cdots w_n$，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 $$\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|w_{i-1})">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide">
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/">
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives">
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags">
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories">
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about">
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank">
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">语言模型和平滑方法</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">语言模型和平滑方法</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-07-31T00:57:52.000Z" itemprop="datePublished" class="page-time">
  2017-07-31
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#语言模型"><span class="post-toc-number">1.</span> <span class="post-toc-text">语言模型</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#模型评估参数"><span class="post-toc-number">2.</span> <span class="post-toc-text">模型评估参数</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#信息量和信息熵"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">信息量和信息熵</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#联合熵和条件熵"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">联合熵和条件熵</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#相对熵和交叉熵"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">相对熵和交叉熵</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#困惑度"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">困惑度</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#数据平滑"><span class="post-toc-number">3.</span> <span class="post-toc-text">数据平滑</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#问题的提出"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">问题的提出</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#加法平滑"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">加法平滑</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Good-Turing"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Good-Turing</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Jelinek-Mercer"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">Jelinek-Mercer</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-nlp-notes" class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">语言模型和平滑方法</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-07-31 08:57:52" datetime="2017-07-31T00:57:52.000Z" itemprop="datePublished">2017-07-31</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style="display:none">
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p><strong>二元语法</strong>$ $</p>
<p>对于一个句子$s=w_1 \cdots w_n$，近似认为一个词的概率只依赖于它前面的<strong>1个词</strong>。即一个状态只跟上一个状态有关，也称为<code>一阶马尔科夫链</code>。</p>
<p>$$<br>\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|w_{i-1})}}<br>$$<br>设$\color {blue} {c(w_{i-1}w_i)}$ 表示二元语法$\color {green} {w_{i-1}w_i}$在给定文本中的出现次数，则上一个词是$w_{i-1}$下一个词是$w_i$的概率$\color {blue} {p(w_i \mid w_{i-1})}$是当前语法$\color {green} {w_{i-1}w_i}$出现的次数比上所有形似$\color {green} {w_{i-1}}w$的二元语法的出现次数<br>$$<br>\color {blue} {p(w_i \mid w_{i-1})} =\color {red} {\frac {c(w_{i-1}w_i)} {\sum_{w} {c(w_{i-1}w)}}}，w是变量<br>$$<br>$n$<strong>元语法</strong></p>
<p>认为一个词出现的概率和它<strong>前面的n个词</strong>有关系。则对于句子$s=w_1w_2 \cdots w_l$，其概率计算公式为如下：</p>
<p>$$<br>\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_1w_2) \cdots p(w_n|w_1w_2 \cdots w_{l-1})=\color {red} {\prod_{i=1}^n{p(w_i|w_1 \cdots w_{i-1})}}<br>$$<br>上述公式需要大量的概率计算，太理想了。一般取$n=2$或者$n=3$。</p>
<p>对于$n&gt;2$的$n$元语法模型，条件概率要考虑前面$n-1$个词的概率，设$w_i^j$表示$w_i\cdots w_j$，则有<br>$$<br>\color{blue} {p(s)} = \prod_{i=1}^{l+1}p(w_i \mid w_{i-n+1}^{i})，\color {blue} {p(w_i \mid w_{i-n+1}^{i})}=\frac { \overbrace {c(w_{i-n+1}^i)}^{\color{red}{具体以w_i结尾的词串w[i-n+1, i]}}} { \underbrace{\sum_{w_i}{c(w_{i-n+1}^i)}}_{\color{red}{所有以w_i结尾的词串w[i-n+1, i]}}}<br>$$<br><strong>实际例子</strong></p>
<p>假设语料库$S$是由下面3个句子组成，所求的句子t在其后：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = [<span class="string">'brown read holy bible'</span>, <span class="string">'plm see a text book'</span>, <span class="string">'he read a book by david'</span>]</span><br><span class="line">t = <span class="string">'brown read a book'</span></span><br></pre></td></tr></table></figure>
<p>那么求句子$t$出现的概率是</p>
<p>$$\color{blue}{p(t)}=p(\color{green}{brown\,read\, a\, book})=p(brown|BOS)p(read|brown)p(a|read)p(book|a)p(eos|book)\approx0.06$$</p>
<p>$n$元文法的一些应用如下</p>
<p><strong>语音识别歧义消除</strong></p>
<p>如给了一个拼音 $\color{green}{ta\,shi \,yan \,jiu \,sheng\, wu\, de}$，得到了很多可能的汉字串：<font color="green">踏实研究生物的，他实验救生物的，他是研究生物的</font> ，那么求出$arg_{str}maxP(str|pinyin)$，即返回最大概率的句子</p>
<p><strong>汉语分词问题</strong></p>
<p>给定汉字串<code>他是研究生物的</code>。可能的汉字串 <code>他 是 研究生 物 的</code>和<code>他 是 研究 生物 的</code>，这也是求最大句子的概率</p>
<p><strong>开发自然语言处理的统计方法的一般步骤</strong></p>
<ul>
<li>收集大量语料（基础工作，工作量最大，<strong>很重要</strong>）</li>
<li>对语料进行统计分析，得出知识（如n元文法，一堆概率）</li>
<li>针对场景建立算法，如计算概率可能也用很多复杂的算法或者直接标注</li>
<li>解释或者应用结果</li>
</ul>
<h1 id="模型评估参数"><a href="#模型评估参数" class="headerlink" title="模型评估参数"></a>模型评估参数</h1><p><strong>基础</strong></p>
<ul>
<li>评价目标：语言模型计   算出的概率分布与“真实的”理想模型是否接近</li>
<li>难点：无法知道“真实的”理想模型的分布</li>
<li>常用指标：交叉熵，困惑度</li>
</ul>
<h2 id="信息量和信息熵"><a href="#信息量和信息熵" class="headerlink" title="信息量和信息熵"></a>信息量和信息熵</h2><p>$X$是一个离散随机变量，取值空间为$R$，其概率分布是$p(x)=P(X=x), x \in R$。</p>
<p><strong>信息量</strong></p>
<p>概率是对事件确定性的度量，那么<strong>信息</strong>就是对<strong>不确定性</strong>的度量。<strong>信息量</strong> $\color {blue} {I(X)}$代表特征的不确定性，定义如下<br>$$<br>\color {blue} {I(X)}= \color {red} {-\log {p(x)}}<br>$$<br><strong>信息熵</strong></p>
<p><strong>信息熵</strong>$\color{blue}{H(x)}$是特征<strong>不确定性的平均值</strong>，用表示，定义如下<br>$$<br>\color{blue}{H(X)}=\sum_{x \in R}{p(x)log\frac 1 {p(x)}}=\color {red} {-\sum_{x \in R} {p(x) \log p(x)}}<br>$$<br>一般是$log_2{p(x)}$，单位是比特。若是$\ln {p(x)}$，单位是奈特。</p>
<ul>
<li>信息熵的本质是信息量的期望</li>
<li><strong>信息熵是对不确定性的度量</strong></li>
<li>随机变量$X$的熵越大，说明不确定性也大；若$X$为定值，则熵为0</li>
<li><code>平均分布</code>是”<strong>最不确定</strong>”的分布</li>
</ul>
<h2 id="联合熵和条件熵"><a href="#联合熵和条件熵" class="headerlink" title="联合熵和条件熵"></a>联合熵和条件熵</h2><p>$X, Y$是一对离散型随机变量，并且$\color{blue}{X,Y  \sim p(x,y)}$。</p>
<p><strong>联合熵</strong></p>
<p>联合熵实际上描述的是一对随机变量平均所需要的信息量，定义如下。<br>$$<br>\color{blue}{H(X, Y)} = \color{red} {- \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y)}<br>$$<br><strong>条件熵</strong></p>
<p>给定$X$的情况下，$Y$的条件熵为<br>$$<br>\color{blue}{H(Y \mid X)} = \color{red}{ - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y \mid x)}<br>$$<br>其中可以推导出：$H(X, Y) = H(X) + H(Y \mid X)$。</p>
<h2 id="相对熵和交叉熵"><a href="#相对熵和交叉熵" class="headerlink" title="相对熵和交叉熵"></a>相对熵和交叉熵</h2><p><strong>相对熵</strong></p>
<p> 随机变量$X$的状态空间$\Omega {x}$上有两个概率分布$p(x)$和$q(x)$。一般p是<code>真实分布</code>，q是<code>预测分布</code>。</p>
<p><strong>相对熵</strong>也称为<code>KL距离</code>，用来衡量相同事件空间里<strong>两个概率分布的差异</strong>。</p>
<p>$p$和$q$的<strong>相对熵</strong>$\color{blue}{D(p\mid\mid q)}$用来度量<strong>它们之间的差异</strong>，如下<br>$$<br>\color{blue}{D(p\mid\mid q)}<br>=\color{red}{\sum_{x\in X} {p(x)\log{\frac {p(x)}{q(x)}}}}<br>= E_p(\log \frac{p(X)}{q(X)}) \; (期望)<br>$$<br>特别地，若$p==q$，则相对熵为0；若差别增加，相对熵的值也增加。简单理解“相对”如下：<br>$$<br>D(p \mid\mid q)=\sum_{x \in X}{p(x)(\log p(x) - \log q(x))}<br>= \underbrace{\left(-\sum_{x \in X}{ \color{red}{p(x)\log q(x)}}\right)}<em>{\color {red}{以q去近似p的熵=交叉熵}} - \underbrace{\left(-\sum</em>{x \in X} {\color{red}{p(x)\log p(x)}}\right)}_{\color{red} {p本身的熵}}<br>$$<br><strong>交叉熵</strong></p>
<p>交叉熵用来衡量<strong>估计模型与真实概率分布之间的差异。</strong></p>
<p>随机变量$X \sim p(x)$，$q(x)$近似于$p(x)$。 则随机变量$X$和模型$q$之间的<strong>交叉熵</strong>$\color {blue} {H(X, q)}$如下：<strong>以$q$去近似$p$的熵</strong><br>$$<br>\color {blue} {H(p, q)} = H(X) + D(p \mid\mid q) = \color {red} {-\sum_{x \in X}{p(x)\log q(x)}}<br>$$</p>
<p><strong>实际应用</strong></p>
<p>交叉熵的实际应用，设$y$是预测的概率分布，$y^\prime$为真实的概率分布。则用<strong>交叉熵去判断估计的准确程度</strong><br>$$<br>H(y^{\prime}, y)= - \sum_i y_i^{\prime}\log y_i = \color {red} {-\sum_i y_{真实} \log y_{预测}}<br>$$<br>  <strong>n元文法模型的交叉熵</strong></p>
<p>设测试集$T=(t_1, t_2, \ldots, t_l)$包含$l$个句子，则定义测试集的概率$\color {blue} {p(T)}$为多个句子概率的乘积<br>$$<br>\color {blue} {p(T)} = \prod_{i=1}^{l} p(t_i)，  \, \text{其中}\color{blue}{p(t_i)}=\color{red}{\prod_{i=1}^{l_w} {p(w_i|w_{i-n+1}^{i-1})}}, \text{见上面}<br>$$<br>其中$w_i^j$表示词$w_i\cdots w_j$，$\sum_{w}{c(read \, w)}$是查找出所有以$read$开头的二元组的出现次数。</p>
<p>则在数据$T$上<code>n元模型</code>$\color {green} {p(w_i|w_{i-n+1}^{i-1})}$的交叉熵$\color {blue} {H_p(T)}$定义如下<br>$$<br>\color {blue} {H_p(T)} = \color {red} {-\frac {1} {W_T} \log _2 p(T)}，其中W_T是文本T中基元(词或字)的长度<br>$$<br>公式的推导过程如下<br>$$<br>-\sum_{x \in X}{p(x)\log q(x)}<br>\implies \underbrace { -{\frac{1} {W_T}}\sum \log q(x)}_{\color{red}{使用均匀分布代替p(x)}}<br>\implies  -{\frac{1} {W_T}} \log {\prod r(w_i|w_{i-n+1}^{i-1})}<br>\implies  -{\frac{1} {W_T}} \log_2p(T)<br>$$<br>可以这么理解：利用模型$p$对$W_T$个词进行编码，每一个编码所需要的平均比特位数。</p>
<h2 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h2><p><strong>困惑度</strong>是评估语言的基本准则人，也是对测试集T中每一个词汇的概率的几何平均值的倒数。<br>$$<br>\color{blue}{PP_T(T)} =\color{red}{ 2^{H_p(T)}= \frac {1} {\sqrt [W_T]{p(T)}}} = 2 ^{\text{交叉熵}}<br>$$</p>
<p>当然，<font color="red">交叉熵和困惑度越小越好。语言模型设计的任务就是要找出困惑度最小的模型。</font></p>
<p>在英语中，n元语法模型的困惑度是$50 \sim 1000$，交叉熵是$6 \sim 10$个比特位。</p>
<h1 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h1><h2 id="问题的提出"><a href="#问题的提出" class="headerlink" title="问题的提出"></a>问题的提出</h2><p>按照上面提出的语言模型，有的句子就没有概率，但是这是不合理的，因为总有出现的可能，概率应该大于0。设$\color {blue}{c(w)}$是$w$在语料库中的出现次数。<br>$$<br>p(\color{green} {read \mid plm}) = \frac {c(plm \mid read)} {\sum_{w_i}{c(plm | w_i})} = \frac {\color{red}{0}} {1}=\color{red}{0， \, 这是不对的}<br>$$<br>因此，必须分配<font color="red">给所有可能出现的字符串一个非0的概率值来避免这种错误的发送。</font></p>
<p><code>平滑技术</code>就是用来解决这种零概率问题的。平滑指的是为了产生更准确的概率来调整最大似然估计的一种技术，也称作<code>数据平滑</code>。思想是<code>劫富济贫</code>，即<strong>提高低概率、降低高概率</strong>，尽量是概率分布趋于均匀。</p>
<p>数据平滑是语言模型中的核心问题</p>
<h2 id="加法平滑"><a href="#加法平滑" class="headerlink" title="加法平滑"></a>加法平滑</h2><p>其实为了避免0概率，最简单的就是给统计次数加1。这里我们可以为每个单词的出现次数加上$\delta，\delta \in [0, 1]$，设$V$是所有词汇的单词表，$|V|$是单词表的词汇个数，则有概率：<br>$$<br>p_{add}(w_i \mid w_{i-n+1}^{i-1}) = \frac {\delta + c(w_{i-n+1}^i)} {\sum_{w_i}{(\delta<em>|V| + c(w_{i-n+1}^i)})}=\frac {\delta + \overbrace {c(w_{i-n+1}^i)}^{\color{red}{具体词串[i-n+1, i]}}} {\delta</em>|V| + \underbrace{\sum_{w_i}{c(w_{i-n+1}^i)}}_{\color{red}{所有以w_i结尾的词串[i-n+1, i]}}}<br>$$</p>
<p>注：这个方法很原始。</p>
<h2 id="Good-Turing"><a href="#Good-Turing" class="headerlink" title="Good-Turing"></a>Good-Turing</h2><p><code>Good-Turing</code>也称作古德-图灵方法，这是很多平滑技术的核心。</p>
<p>主要思想是重新分配概率，会得到一个<code>剩余概率量</code>$\color {blue} {p_0}= \color {red} {\frac {n_1} N}$，设$n_0$为未出现的单词的个数，然后<font color="red">由这$n_0$个单词去平均分配得到$p_0$</font>，即每个未出现的单词的概率为$\frac {p_0} {n_0}$。</p>
<p>对于一个$n$元语法，设$\color {blue} n_r$恰好出现$r$次的$n$元语法的数目，下面是一些新的定义</p>
<ul>
<li>出现次数为$r$的$n$元语法 新的出现次数$\color {blue} {r^*} = \color {red} {(r+1)\frac{n_{r+1}}{n_r}}$</li>
<li>设$N = \sum_{r=0}^{\infty}n_r r^* = \sum_{r=1}^{\infty} n_r r$，即$N$是这个分布中最初的所有文法出现的次数，例如所有以$read$开始的总次数</li>
<li>出现次数为$r$的修正概率 $\color {blue}p_r = \color {red} {\frac {r^*} {N}}$</li>
</ul>
<p>剩余概率量$\color {blue} {p_0}= \color {red} {\frac {n_1} N}$的推导<br>$$<br>总的概率 = \sum_{r&gt;0}{n_r p_r} = \sum_{r&gt;0}{n_r  (r+1)\frac{n_{r+1}}{n_r N}} = \frac {1}{N} (\sum_{r&gt;0} (r+1)n_{r+1} =  \frac {1}{N} (\sum_{r&gt;0} (r n_r - n_1) = 1 - \frac {n_1} N &lt; 1<br>$$<br>然后把$p_0$平均分配给所有未见事件(r=0的事件)。</p>
<p><strong>缺点</strong></p>
<ul>
<li>若出现次数最大为$k$，则无法计算$r=k$的新的次数$r^*$和修正概率$p_r$</li>
<li>高低阶模型的结合通常能获得较好的平滑效果，但是Good-Turing不能高低阶模型结合</li>
</ul>
<h2 id="Jelinek-Mercer"><a href="#Jelinek-Mercer" class="headerlink" title="Jelinek-Mercer"></a>Jelinek-Mercer</h2><p><strong>问题引入</strong></p>
<p>假如$c(send \, the)=c(send \, thou)=0$，则通过GT方法有$p(the \mid send)=p(thou \mid send)$，但是实际上却应该是$p(the \mid send)&gt;p(thou \mid send)$。</p>
<p>所以我们需要在二元语法模型中加入一个一元模型<br>$$<br>p_{ML}(w_i) = \frac {c(w_i)}{\sum_w{c(w)}}<br>$$<br><strong>二元线性插值</strong></p>
<p>使用$r$将二元文法模型和一元文法模型进行线性插值<br>$$<br>p(w_i \mid w_{i-1}) = \lambda p_{ML}(w_i | w_{i-1}) + (1-\lambda)p_{ML}(w_i)，\lambda \in [0, 1]<br>$$<br>所以可以得到$p(the \mid send)&gt;p(thou \mid send)$</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-11-25T08:30:10.541Z" itemprop="dateUpdated">2018-11-25 16:30:10</time>
</span><br>


        
        <br>原始链接：<a href="/2017/07/31/nlp-notes/" target="_blank" rel="external">http://plmsmile.github.io/2017/07/31/nlp-notes/</a>
        
    </div>
    
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/各种熵/">各种熵</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据平滑/">数据平滑</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/自然语言处理/">自然语言处理</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/语言模型/">语言模型</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&source=NLP, DL, MRC." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《语言模型和平滑方法》 — PLM's Notes&url=http://plmsmile.github.io/2017/07/31/nlp-notes/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/08/04/pgm-01/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">马尔可夫模型</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/07/29/aim2offer/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">剑指Offer算法题</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "kR8nND4dcsWgqDWIjpiH4YFj-gzGzoHsz",
            appKey: "il7PLkcJCfDBXMR6XirLdO2K",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->







</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check" data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="bottom">
        
<p>
    <span id="busuanzi_container_site_uv" style="display:none">
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            <span>
            PLM's Notes &nbsp; &copy; &nbsp
            </span>
            2016 - 2018
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&source=NLP, DL, MRC." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《语言模型和平滑方法》 — PLM's Notes&url=http://plmsmile.github.io/2017/07/31/nlp-notes/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACJUlEQVR42u3aS26EMBAFQO5/6ck2UoJ5bZuRaIpVlBhwEanl/hxHfH2G1+81Z+vHT/5713HHhYGB8VhGvsWzF4w3fUZNnpDsDQMD4z2MJBQm+Oqa8buS9RgYGBjjNXm4zI+MGBgYGHczqsjxczAwMDDWS2krsPxouDkXx8DAeCAjr7p//+db+hsYGBiPYnyKV5KarifA5V1hYGC0ZuQBrlogy585F3AxMDDew8jTxWo6uo4Zf5TT/wMGBkZTRl6Or4bX6meaK8NhYGD0ZswNT1TX5AF0WwMAAwOjEeOOAlmeAM8V6f5Zg4GB8RpGvt08vVzfenRsxcDAeAHjopi1tTCX/LWaGGNgYPRm5CWtccDNt76CuXgOBgbGCxhzBfq8TL/eHL14FwYGRlNGEkbHYa7aNpgLx4VWJQYGRjvG3JDWenEt/3yTwxYYGBiNGHOl/7lxjfxp5WCNgYHRmrErBa2OU1R/EyExMDBaM/LhrWro3Eu6GLnAwMBozcjLZ/ldc4E4aQ9MDltgYGA0YuRHsb0DFtVhiyPveWJgYLRjVBPIvOm4oTGZnG0xMDDaMT7FKw/B1RLeSojHwMDozVgvlt3xhDypLsMwMDAey6iOZK1vMS/GlZNYDAyM1ow88M2VyZIPsS0Xx8DAwAgY47s2Ny8xMDAw4jx4/fhYbTNEwxYYGBiNGNVmQPLK6l358REDA+NtjGrquD5yMZfWbmhqYmBgPI/xA50mQc8gIkNEAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
